{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymdptoolbox in c:\\users\\groth\\cond\\lib\\site-packages (4.0b3)\n",
      "Requirement already satisfied: numpy in c:\\users\\groth\\cond\\lib\\site-packages (from pymdptoolbox) (1.23.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\groth\\cond\\lib\\site-packages (from pymdptoolbox) (1.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymdptoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example, mdptoolbox.util\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Example: Forest Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The considered problem is to manage a forest stand with first the objectiveto maintain an old forest for wildlife and second to make money selling cutwood. \n",
    "\n",
    "The forest stand is managed by two possible actions: Wait (action 0) or Cut (action 1). An action is decided and applied each time period  at the beginning of the period.\n",
    "\n",
    "Three states are defined, corresponding to 3 age-class of trees: age-class 0-20years (state 1), 21-40 years (state 2), more than 40 years (state 3).  Thestate 3 correspond to the oldest age-class. \n",
    "\n",
    "At the end of a period t, if the state was s at t and action Wait   is choosen, the state at the next time period will be min(s+ 1,3) if no fire occured (the forest grows). But there is a probability p that a fire burns the forest after the application of the action, living the stand at  the youngest age-class (state 1). \n",
    "\n",
    "Let p = 0.1 be the probability of wildfire occurence during a time period. The problem is how to manage this stand in a long term vision to maximizethe γ-discounted reward with γ= 0.95\n",
    "\n",
    "\n",
    "mdptoolbox.example.forest() provides  a modelisation of this problem in the MDP framework. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Q1. create and check  a mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    " # Creating a forest management problem with 3 states, a direct cut reward equal to 1 but for the last state (where it is equal to 2)\n",
    "# a preservation reward equal to 4, and a probability of fire equal to 0.1\n",
    "#(for more details,  look  at the \"example\" module)\n",
    "    \n",
    "P, R = mdptoolbox.example.forest(3,4,2,0.1)\n",
    "\n",
    "# the following call checks that the MDP is correctly defined \n",
    "#(for more details,  look  at the \"util\" module : https://pymdptoolbox.readthedocs.io/en/latest/api/util.html)\n",
    "\n",
    "mdptoolbox.util.check(P, R) # Nothing should happen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Observe the transition matrix (P)  and the reward matrix (R)   \n",
    " \n",
    " In particular check  probability to be in state 3 at time t+1 (s’ = 3), when being instate 3 (s = 3) and choosing action Wait (a = 0) at time t, is P(3,3,1) = 1 -p = 0.9 and the associated reward is R(3,1) = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.1, 0.9, 0. ],\n",
       "        [0.1, 0. , 0.9],\n",
       "        [0.1, 0. , 0.9]],\n",
       "\n",
       "       [[1. , 0. , 0. ],\n",
       "        [1. , 0. , 0. ],\n",
       "        [1. , 0. , 0. ]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [4., 2.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Policy iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the policy iteration algorithm and look at  the policy computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration\t\tNumber of different actions\n",
      "    1\t\t  1\n",
      "    2\t\t  0\n",
      "Iterating stopped, unchanging policy found.\n"
     ]
    }
   ],
   "source": [
    "# Apply the policy iteration algorithm\n",
    "P, R = mdptoolbox.example.forest(3,4,2,0.1)\n",
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
    "pi.setVerbose()\n",
    "pi.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the policy associated to pi\n",
    "pi.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26.244000000000014, 29.484000000000016, 33.484000000000016)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display  its value \n",
    "\n",
    "pi.V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the minimal for probabilty of fire shall  that implies that it is better to cut   the forest  than wait in state 2 (age 21-40)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration\t\tNumber of different actions\n",
      "    1\t\t  1\n",
      "    2\t\t  0\n",
      "Iterating stopped, unchanging policy found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P, R = mdptoolbox.example.forest(3,4,2,0.1)\n",
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
    "pi.setVerbose()\n",
    "pi.run()\n",
    "pi.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. Value iteration\n",
    "\n",
    "Look at https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html#mdptoolbox.mdp.ValueIteration\n",
    "    \n",
    "And apply the Value iteration to the same problem (once with p=0.1, once with p=0.79) \n",
    "\n",
    "You can modify the  epsilon threshold and observe the number of iteration needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration\t\tV-variation\n",
      "    1\t\t  4.0\n",
      "    2\t\t  2.5029\n",
      "    3\t\t  0.9122714100000007\n",
      "    4\t\t  0.0489874447889993\n",
      "    5\t\t  0.01884470075378175\n",
      "    6\t\t  0.02190519408847269\n",
      "    7\t\t  0.019937784643719425\n",
      "    8\t\t  0.017800582030833567\n",
      "    9\t\t  0.01586472175726783\n",
      "    10\t\t  0.01413712536759526\n",
      "    11\t\t  0.012597471054032638\n",
      "    12\t\t  0.01122548314916827\n",
      "    13\t\t  0.010002916915990312\n",
      "    14\t\t  0.008913500145983022\n",
      "    15\t\t  0.007942731648864054\n",
      "    16\t\t  0.007077689460533776\n",
      "    17\t\t  0.006306858938941673\n",
      "    18\t\t  0.005619979500028904\n",
      "    19\t\t  0.005007908038937359\n",
      "    20\t\t  0.004462497225517836\n",
      "    21\t\t  0.003976487054679012\n",
      "    22\t\t  0.0035434082077507867\n",
      "    23\t\t  0.0031574959390354707\n",
      "    24\t\t  0.0028136133407379305\n",
      "    25\t\t  0.0025071829652532074\n",
      "    26\t\t  0.0022341258943576747\n",
      "    27\t\t  0.0019908074444501267\n",
      "    28\t\t  0.0017739887850112268\n",
      "    29\t\t  0.001580783826231169\n",
      "    30\t\t  0.0014086208021169\n",
      "    31\t\t  0.0012552080374490515\n",
      "    32\t\t  0.0011185034431555607\n",
      "    33\t\t  0.0009966873339166682\n",
      "Iterating stopped, epsilon-optimal policy found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25.5833879767579, 28.830654635546928, 32.83065463554693)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P, R = mdptoolbox.example.forest(3,4,2,0.1)\n",
    "pi = mdptoolbox.mdp.ValueIterationGS(P, R, 0.9, 0.01)\n",
    "pi.setVerbose()\n",
    "pi.run()\n",
    "pi.policy\n",
    "pi.V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4.   Setting the parameters\n",
    "\n",
    "\n",
    "\n",
    "You can also have a look at the cpu time neeed for the last run of the algorithm ; \n",
    "\n",
    "Considering a bigger problem (ex: 100 states),\n",
    "\n",
    "P, R = mdptoolbox.example.forest(100,4,2,0.1)\n",
    "\n",
    "which is the best policy (when to begin the cut ?)\n",
    "\n",
    "which is the average time (on 10 runs) for each algo ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "P, R = mdptoolbox.example.forest(100,4,2,0.1)\n",
    "#pi = mdptoolbox.mdp.ValueIterationGS(P, R, 0.9)\n",
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
    "pi.run()\n",
    "#pi.policy\n",
    "#pi.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating an instance of  ValueIterationGS  or of PolicyIteration, you can play with the parameters of the solving\n",
    "\n",
    "\n",
    "class mdptoolbox.mdp.ValueIterationGS(transitions, reward, discount, epsilon=0.01, max_iter=10, initial_value=0, skip_check=False)\n",
    "\n",
    "class mdptoolbox.mdp.PolicyIteration(transitions, reward, discount, policy0=None, max_iter=1000, eval_type=0, skip_check=False)\n",
    "\n",
    "(look again at https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html)\n",
    "\n",
    "\n",
    "Which is the best parametrization to have the quickest solving with   epsilon=0.01 ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfa8ce728e9d9cc48035c561846d2d58eda711f129329f3ea16e216e9713e258"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
